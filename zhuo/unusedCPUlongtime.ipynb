{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b88e4-4499-4a6b-b5bc-dbcc6748efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect unused CPU by running jobs for a long time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "# 1. Function to convert epoch to a CST datetime object\n",
    "def convert_epoch_to_cst_datetime(epoch, cst):\n",
    "    utc_time = datetime.utcfromtimestamp(epoch).replace(tzinfo=pytz.utc)\n",
    "    return utc_time.astimezone(cst)\n",
    "\n",
    "# 2. Function to create 'submitInterval' column based on time of day\n",
    "def get_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "# 3. Function to read CSV file\n",
    "def read_csv(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# 4. Function to convert 'submitTime', 'startTime', and 'endTime' to CST\n",
    "def convert_time_columns(df, cst):\n",
    "    df['submitTime'] = df['submitTime'].apply(lambda x: convert_epoch_to_cst_datetime(x, cst))\n",
    "    df['startTime'] = df['startTime'].apply(lambda x: convert_epoch_to_cst_datetime(x, cst))\n",
    "    df['endTime'] = df['endTime'].apply(lambda x: convert_epoch_to_cst_datetime(x, cst))\n",
    "    return df\n",
    "\n",
    "# 5. Function to calculate 'pendTime'\n",
    "def calculate_pend_time(df):\n",
    "    df['pendTime'] = (df['startTime'] - df['submitTime']).dt.total_seconds()\n",
    "    return df\n",
    "\n",
    "# 6. Function to add 'submitInterval' and 'submitDay' columns\n",
    "def add_time_of_day_and_day(df):\n",
    "# Define the desired order for submitDay and submitInterval\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    interval_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "    \n",
    "    # Add submitInterval and submitDay columns\n",
    "    df['submitInterval'] = df['submitTime'].dt.hour.apply(get_time_of_day)\n",
    "    df['submitDay'] = df['submitTime'].dt.day_name()\n",
    "    \n",
    "    # Set categorical ordering for submitDay and submitInterval\n",
    "    df['submitDay'] = pd.Categorical(df['submitDay'], categories=day_order, ordered=True)\n",
    "    df['submitInterval'] = pd.Categorical(df['submitInterval'], categories=interval_order, ordered=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 7. Function to overwrite 'queue' after replacing 'rhel8_' and 'rhel88_'\n",
    "def overwrite_queue(df):\n",
    "    df['queue'] = df['queue'].str.replace(r'rhel8_', '', regex=True)\n",
    "    df['queue'] = df['queue'].str.replace(r'rhel88_', '', regex=True)\n",
    "    return df\n",
    "\n",
    "# 8. Function to extract memory from 'resReq' and overwrite existing values\n",
    "def extract_requested_memory(df):\n",
    "    df['resReq'] = df['resReq'].astype(str)\n",
    "    df['requested_memory'] = df['resReq'].apply(lambda resReq: float(re.search(r'mem=(\\d+(\\.\\d+)?)', resReq).group(1)) if re.search(r'mem=(\\d+(\\.\\d+)?)', resReq) else None)\n",
    "    return df.dropna(subset=['requested_memory'])\n",
    "\n",
    "# 9. Function to convert 'maxRMem' from KB to MB\n",
    "def convert_max_rmem(df):\n",
    "    df['maxRMem'] = df['maxRMem'] / 1024\n",
    "    return df\n",
    "\n",
    "# 10. Function to create the 'userCancelJob' indicator variable\n",
    "def create_user_cancel_job(df):\n",
    "    df['userCancelJob'] = np.where(df['pendTime'] < 0, 1, 0)\n",
    "    return df\n",
    "\n",
    "# 11. Function to filter rows where 'userCancelJob' is greater than 0\n",
    "def remove_user_cancel_job(df):\n",
    "    return df[df['userCancelJob'] == 0]\n",
    "\n",
    "# 12. Function to order the 'queue' variable based on value counts\n",
    "def order_queue_by_value_counts(df):\n",
    "    queue_counts = df['queue'].value_counts()\n",
    "    ordered_queues = queue_counts.index\n",
    "    df['queue'] = pd.Categorical(df['queue'], categories=ordered_queues, ordered=True)\n",
    "    return df\n",
    "\n",
    "#13. Define Job category based on Job distribution\n",
    "def define_job_category(df):\n",
    "    # Calculate percentiles for runTime\n",
    "    df_done = df[df['jStatus'] == 'done']\n",
    "    short_threshold = df['runTime'].quantile(0.75)\n",
    "    long_threshold = df['runTime'].quantile(0.95)\n",
    "    verylong_threshold = df['runTime'].quantile(0.99)\n",
    "    # Define job categories based on runTime\n",
    "    conditions = [\n",
    "        (df['runTime'] <= short_threshold),\n",
    "        (df['runTime'] > short_threshold) & (df['runTime'] <= long_threshold),\n",
    "        (df['runTime'] > long_threshold) & (df['runTime'] <= verylong_threshold),\n",
    "        (df['runTime'] > verylong_threshold)\n",
    "    ]\n",
    "    \n",
    "    # Assign labels to the categories\n",
    "    categories = ['Short', 'Medium', 'Long', 'Very Long']\n",
    "    \n",
    "    # Create a new column 'jobCategory' based on the conditions\n",
    "    df['jobCategory'] = np.select(conditions, categories)\n",
    "    \n",
    "    # Ensure 'jobCategory' is an ordered categorical variable\n",
    "    df['jobCategory'] = pd.Categorical(df['jobCategory'], \n",
    "                                       categories=['Short', 'Medium', 'Long', 'Very Long'],\n",
    "                                       ordered=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "###14. Calculate the memory utilization \n",
    "def calculate_and_categorize_memutilization(df):\n",
    "    # Calculate memory utilization\n",
    "    df.loc[df['numProcessors'] > 0, 'memUtilization'] = (df['maxRMem'] / (df['numProcessors'] * df['requested_memory'])) * 100\n",
    "    df.loc[df['numProcessors'] == 0, 'memUtilization'] = 0\n",
    "\n",
    "    # Replace inf and nan with 0\n",
    "    df['memUtilization'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    df['memUtilization'].fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate percentiles\n",
    "    percentiles = df['memUtilization'].quantile([0.50, 0.75, 0.95])\n",
    "    \n",
    "    # Define categories based on percentiles\n",
    "    def categorize_utilization(utilization):\n",
    "        if utilization <= percentiles[0.50]:\n",
    "            return 'Low'\n",
    "        elif utilization <= percentiles[0.75]:\n",
    "            return 'Medium'\n",
    "        elif utilization <= percentiles[0.95]:\n",
    "            return 'High'\n",
    "        else:\n",
    "            return 'Very High'\n",
    "    \n",
    "    # Apply categorization\n",
    "    df['memUtilizationCat'] = df['memUtilization'].apply(categorize_utilization)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 15. Main preprocessing function\n",
    "def modelProcess_csv(file_path):\n",
    "    cst = pytz.timezone('US/Central')  # Define CST timezone    \n",
    "    # Step-by-step preprocessing\n",
    "    df = read_csv(file_path)\n",
    "    df = convert_time_columns(df, cst)\n",
    "    df = calculate_pend_time(df)\n",
    "    df = add_time_of_day_and_day(df)\n",
    "    df = overwrite_queue(df)\n",
    "    df = extract_requested_memory(df)\n",
    "    df = convert_max_rmem(df)\n",
    "    df = create_user_cancel_job(df)\n",
    "    df = remove_user_cancel_job(df)  # Keep only rows where userCancelJob > 0\n",
    "    df = order_queue_by_value_counts(df)  # Order the 'queue' column by value counts\n",
    "    df = define_job_category(df) #Categorize job to short/intermediate/long\n",
    "    df = calculate_and_categorize_memutilization(df) ## calculate and categorize memory utilization\n",
    "\n",
    "    # Display the filtered dataframe\n",
    "    #print(df.head())\n",
    "    #print(df.shape[0])\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a52c3-b931-4839-a4e5-1ba26c883588",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../common/finished_jobs_1_week.csv'\n",
    "df_preprocessed = modelProcess_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97915db-52fe-42b0-9337-bb1999bf6d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
